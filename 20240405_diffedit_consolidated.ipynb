{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms as tfms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, logging\n",
    "import os\n",
    "from typing import List, Dict, Any, Tuple, Union\n",
    "\n",
    "torch.manual_seed(1)\n",
    "if not os.path.exists(f'{os.path.expanduser(\"~\")}/.cache/huggingface/token'):\n",
    "    notebook_login()\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data_root = './data/'\n",
    "torch.set_printoptions(precision=3, linewidth=140, sci_mode=False)\n",
    "np.set_printoptions(precision=3, linewidth=140)\n",
    "\n",
    "# Ignore UserWarnings from PyTorch about CUDA capabilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n",
    "\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "\n",
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_to_latent(img):\n",
    "    img_inp = tfms.ToTensor()(img)[None].to(torch_device) * 2 - 1\n",
    "    with torch.no_grad():\n",
    "        latent = vae.encode(img_inp)\n",
    "    return latent.latent_dist.sample() * 0.18215\n",
    "\n",
    "def latents_to_pil(latents):\n",
    "    with torch.no_grad():\n",
    "        images = vae.decode(latents/0.18215).sample\n",
    "    images = (images * 0.5 + 0.5).clamp(0, 1)\n",
    "    images = images.permute(0, 2, 3, 1)\n",
    "    images = (images.cpu().numpy() * 255).round().astype('uint8')\n",
    "    image_list = [Image.fromarray(img) for img in images]\n",
    "    return image_list\n",
    "\n",
    "def show_latents(latents_org: torch.tensor):\n",
    "    latents = latents_org.clone()\n",
    "    if len(latents.shape) == 3:\n",
    "        latents = latents[None]\n",
    "    if len(latents.shape) != 4:\n",
    "        raise ValueError(\"latents must be of shape (num_latents, num_channels, height, width) or (num_channels, height, width)\")\n",
    "    latents =latents.detach().cpu().numpy()\n",
    "    N, C, _, _ = latents.shape\n",
    "    fig, axs = plt.subplots(N, C, figsize=(4*C, 4*N))\n",
    "    if N == 1:\n",
    "        axs = axs[None]\n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            axs[n,c].imshow(latents[n,c], cmap='Greys_r')\n",
    "            axs[n,c].set_title(f\"Latent {n},{c}\")\n",
    "            axs[n,c].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def encode_text(prompts):\n",
    "    text_input = tokenizer(prompts, padding='max_length', max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        text_embed = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "    return text_embed\n",
    "\n",
    "def torch_percentile(T, q):\n",
    "    k = 1 + round(.01 * float(q) * (T.numel() - 1))\n",
    "    result = T.view(-1).kthvalue(k).values.item()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diffedit_mask(image: Union[Image.Image, str], prompts: List[str],  start_step: int = 30, \n",
    "    num_timesteps: int = 50, outlier_percentile: int = 10, num_iterations: int = 10) -> Union[np.ndarray, torch.tensor]:\n",
    "    if isinstance(image, str):\n",
    "        image = Image.open(image).convert('RGB').resize((512, 512))\n",
    "    else:\n",
    "        image = image.resize((512, 512))\n",
    "    latents = pil_to_latent(image)\n",
    "    text_embeds = encode_text(prompts)\n",
    "    scheduler.set_timesteps(num_timesteps)\n",
    "    time_start_step = scheduler.timesteps[start_step]\n",
    "    masks = []\n",
    "    for iterid in range(num_iterations):\n",
    "        noise = torch.randn_like(latents)\n",
    "        latent_noised = scheduler.add_noise(latents, noise, timesteps=torch.tensor([time_start_step]))\n",
    "        latent_noised_batch = torch.cat([latent_noised]*len(prompts))\n",
    "        latent_noised_batch_scaled = scheduler.scale_model_input(latent_noised_batch, time_start_step)\n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(latent_noised_batch_scaled, time_start_step, encoder_hidden_states=text_embeds)[\"sample\"]\n",
    "        org, new = noise_pred.chunk(2)\n",
    "        org = org[0,...]\n",
    "        new = new[0,...]\n",
    "        diff = (org - new).abs().sum(dim=0)\n",
    "        outlier_max = torch_percentile(diff, 100 - outlier_percentile)\n",
    "        outlier_min = torch_percentile(diff, outlier_percentile)\n",
    "        diff = diff.clamp(outlier_min, outlier_max)\n",
    "        diff = (diff - diff.min()) / (diff.max() - diff.min())\n",
    "        masks.append(diff[None])\n",
    "    diff_avg = torch.cat(masks).mean(dim=0)\n",
    "    diff_binary = (diff_avg > 0.5).float()\n",
    "    return diff_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diffedit_image(image: Image.Image, mask: torch.tensor, prompts: List[str], start_step: int = 20, num_timesteps: int = 50, guidance: float = 7.5) -> Image.Image:\n",
    "    text_embeds = encode_text(prompts)\n",
    "    latents = pil_to_latent(image)\n",
    "    scheduler.set_timesteps(num_timesteps)\n",
    "    time_start_step = scheduler.timesteps[start_step]\n",
    "    latents_curr = scheduler.add_noise(latents, torch.randn_like(latents), timesteps=torch.tensor([time_start_step]))\n",
    "    for step_id in range(start_step, num_timesteps):\n",
    "        t = scheduler.timesteps[step_id]\n",
    "        latents_curr_batch = torch.cat([latents_curr]*len(prompts))\n",
    "        latents_curr_batch_scaled = scheduler.scale_model_input(latents_curr_batch, t)\n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(latents_curr_batch_scaled, t, encoder_hidden_states=text_embeds)[\"sample\"]\n",
    "        u_pred, t_pred  = noise_pred.chunk(2)\n",
    "        t_pred = u_pred + guidance * (t_pred - u_pred)\n",
    "        latents_curr = scheduler.step(t_pred, t, latents_curr).prev_sample\n",
    "        if step_id < num_timesteps - 1:\n",
    "            t_minus_1 = scheduler.timesteps[step_id + 1]\n",
    "            latents_org = scheduler.add_noise(latents, torch.randn_like(latents), timesteps=torch.tensor([t_minus_1]))\n",
    "            latents_curr = mask * latents_curr + (1 - mask) * latents_org\n",
    "    image_transformed = latents_to_pil(latents_curr)[0]\n",
    "    return image_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(f'{data_root}/misc_images/horse.png').convert('RGB').resize((512, 512))\n",
    "from_text = \"a horse\"\n",
    "to_text = \"a zebra\"\n",
    "mask_start_step = 30\n",
    "mask_num_timesteps = 50\n",
    "mask_outlier_percentile = 10 # top and bottom percentile to remove outliers\n",
    "mask_num_iterations = 10 # Number of iterations to average the masks over\n",
    "edit_start_steps = 20\n",
    "edit_num_timesteps = 50\n",
    "edit_guidance = 7.5\n",
    "\n",
    "mask = get_diffedit_mask(image, [from_text, to_text], start_step=mask_start_step, \n",
    "        num_timesteps=mask_num_timesteps, outlier_percentile=mask_outlier_percentile, num_iterations=mask_num_iterations)\n",
    "\n",
    "image_edited = get_diffedit_image(image, mask, [\"\", to_text], start_step=edit_start_steps, \n",
    "        num_timesteps=edit_num_timesteps, guidance=edit_guidance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30, 10))\n",
    "ax1.imshow(image); ax1.set_title(\"Input Image\")\n",
    "ax2.imshow(mask.cpu().numpy(), cmap='Greys_r'); ax2.set_title(f'Mask: |\"{from_text}\" - \"{to_text}\"|')\n",
    "ax3.imshow(image_edited); ax3.set_title(\"Edited Image\")\n",
    "ax1.axis('off'); ax2.axis('off'); ax3.axis('off'); "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
